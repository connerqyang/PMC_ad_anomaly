# -*- coding: utf-8 -*-
"""imputations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QycPyw79CF4K2YJQtwQ2r-QvMhPmje_-

This python library that should be run on an original dataset such as PMC_standard.csv to clean up the data to be ready to be run in the algorithm
"""

#SETTING UP

#Import neccessary libraries
import pandas as pd
import numpy as np
import os
from datetime import datetime
from google.colab import drive
drive.mount('/content/gdrive')
DATASET_PATH = '/content/gdrive/My Drive/datasets/PMC/PMC_standard.csv'
SAVE_PATH = '/content/gdrive/My Drive/datasets/PMC/PMC_with_imputed_adunits.csv'

#Reading the dataset in a dataframe using Pandas
df = pd.read_csv(DATASET_PATH, index_col=0)
original = df

#Drop unneccesary columns
print("Below is an example of what the inputted data looks like:")
df.head()
#This outputs an example of what the data should look like. It is highly important that the dates are from most recent (top) to oldest (bottom).

"""Find missing days for adundits and create rows for them with 0 sumrevenue, starting from the adunit's date of first occurrence"""

#Find missing days for adundits and create rows for them with 0 sumrevenue, starting from the adunit's date of first occurrence
import itertools

df['date'] = pd.to_datetime(df['date']) #Before, we start, change 'date' column to a dateTime object that can be used to compare dates to dates (strings cannot do this as easily)
webToDateDict = {}                      #First create a dictionary mapping each adunit to its date of first occurrence
df = df.iloc[::-1]                      #Reverse the order of the dataframe so that we can add the earliest date that a adunit (key) appears in the dataframe

  #Iterate through each row, associating the adunit (row[0]) as the key, to the date (row[adunit]) as the value if it does not already exist in the dictionary. 
  #Since the dataframe is in reverse order (Starting from earliest date, sorted by date), it should only associate each adunit with its date of first appearance.
for index, row in df.iterrows():
  adunit = row['adunit']
  date = row['date'];

  #Only want to associate the date if it is the first one.
  if webToDateDict.get(adunit) == None:
    webToDateDict[adunit] = date;

#Split each 'adunit' into a unique dataframe, reindex (pandas) by date to fill in missing dates, then concatenate all dataframe back together
df_list = []
for website in df['adunit'].unique():
    #Check to make sure that we do not reindex for the 'Unknown' websites, we do not want to impute missing data for unknown websites
    if website == 'Unknown':
      continue

    tmp_df = df.loc[df['adunit'] == website].copy()   #Create a temporary dataframe of the current unique website

    #Reindexing by 'date' will fill in missing values of 0 for all missing dates for this dataframe
    idx = pd.date_range(start=df.date.min(), end=df.date.max())
    tmp_df = tmp_df.set_index('date').reindex(idx, fill_value=0).rename_axis('date').reset_index().copy()
    tmp_df['adunit'] = website    #Assign the correct website name for the 'adunit' column

    df_list.append(tmp_df)        #Append the temporary dataframe for each individual 'adunit' into a list of dataframe

df = pd.concat(df_list).copy()    #Concatenate all dataframes in the list into one combined dataframe of all 'adunits' again

#Creates a 'badDate' column. If this column's value is 1, this 'adunit' has a bad date, which means it should be removed
df.reset_index(inplace=True)
df = df.drop(['index'], axis=1)
df['badDate'] = 0;

#Finally, we will remove the dates that are before the first date of appearance of an adunit
#We do so using the dictionary that maps adunit to first date, and flagging and removing dates that are before the first date
for index, row in df.iterrows():
  if row['date'].date() < webToDateDict[row['adunit']].date():
    df['badDate'][index] = 1
  print(index)

df = df[df.badDate == 0]              #Create a new dataframe out of the non-flagged rows, thus removing all of the bad dates
df = df.drop(columns = ['badDate'])   #Resets dataframe back to original state (minus the revmean and revmed columns, whose values are affected by imputations)

#Export dataframe to a .csv file, for use in other algorithms
df.to_csv(SAVE_PATH)

#Print number of rows in old, new dataset, and difference
print('Length of original data is', str(len(original.index)))
print('Length of new data is', str(len(df.index)))
print(str(len(df.index) - len(original.index)), 'rows of data were imputed')